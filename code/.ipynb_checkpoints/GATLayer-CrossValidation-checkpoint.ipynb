{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e0d4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "# PyTorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.nn import GINConv, global_add_pool, GCNConv, global_mean_pool\n",
    "import torch_geometric\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv, TransformerConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#Sklearn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Networkx\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35511c2",
   "metadata": {},
   "source": [
    "## 1. Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5caf6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_features(graph, experiment):\n",
    "     \n",
    "    # Obtener los nodos y sus características\n",
    "    nodes = list(graph.nodes())\n",
    "    \n",
    "     # Calcular las características de los nodos\n",
    "        \n",
    "    if experiment == 1: \n",
    "        clustering_coeffs = nx.clustering(graph)\n",
    "        x = torch.tensor([[clustering_coeffs[node]] for node in nodes], dtype=torch.float)\n",
    "        \n",
    "    elif experiment == 2: \n",
    "        clustering_coeffs = nx.clustering(graph)\n",
    "        degree = nx.degree(graph)\n",
    "        x = torch.tensor([[clustering_coeffs[node], degree[node]] for node in nodes], dtype=torch.float)\n",
    "        \n",
    "    else:\n",
    "        clustering_coeffs = nx.clustering(graph)\n",
    "        degree = nx.degree(graph)\n",
    "        pagerank = nx.pagerank(graph)\n",
    "        x = torch.tensor([[clustering_coeffs[node], degree[node],pagerank[node]] for node in nodes], dtype=torch.float)\n",
    "\n",
    "    return x\n",
    "\n",
    "def load_gpickle_files(path, experiment):\n",
    "    X_path = os.path.join(path, \"X\")  # Ruta de la carpeta con los grafos\n",
    "    y_path = os.path.join(path, \"y\")  # Ruta de la carpeta con las etiquetas\n",
    "\n",
    "    file_list = os.listdir(X_path)  # Obtén la lista de archivos gpickle\n",
    "    dataset = []\n",
    "\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(X_path, file)  # Genera la ruta del archivo\n",
    "        graph = nx.read_gpickle(file_path)  # Lee el archivo gpickle con NetworkX\n",
    "\n",
    "        # Calcular las características de los nodos\n",
    "        x = calculate_node_features(graph, experiment)\n",
    "        # Obtiene la matriz dispersa de adyacencia\n",
    "        adj_matrix = nx.convert_matrix.to_scipy_sparse_matrix(graph)\n",
    "\n",
    "        # Convierte la matriz dispersa en un tensor de PyTorch\n",
    "        edge_index = torch.from_numpy(np.vstack(adj_matrix.nonzero()))\n",
    "#         edge_index = torch.tensor(list(graph.edges()), dtype=torch.long).t().contiguous()  # Índices de las aristas\n",
    "#         adj_matrix = nx.adjacency_matrix(graph)\n",
    "#         adj_matrix = adj_matrix.toarray()\n",
    "\n",
    "        file_number = os.path.splitext(file)[0]\n",
    "        with open(os.path.join(y_path, f\"{file_number}.txt\")) as f:\n",
    "            target = f.read()\n",
    "            y = torch.tensor(np.float_(target), dtype=torch.float)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)  # Crea un objeto Data\n",
    "        dataset.append(data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Extraer los elementos del lote y crear listas separadas para cada atributo\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for data in batch:\n",
    "        x_list.append(data.x)\n",
    "        edge_index_list.append(data.edge_index)\n",
    "        y_list.append(data.y)\n",
    "\n",
    "    # Convertir las listas en arreglos de numpy\n",
    "    x_batch = np.stack(x_list)\n",
    "    edge_index_batch = np.stack(edge_index_list)\n",
    "    y_batch = np.stack(y_list)\n",
    "\n",
    "    return Data(x=x_batch, edge_index=edge_index_batch, y=y_batch)\n",
    "\n",
    "def plot_learning_curves(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot losses\n",
    "    plt.plot(epochs, train_losses, label='Train')\n",
    "    plt.plot(epochs, val_losses, label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Curva de Aprendizaje')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, num_epochs, dataset, data_test):\n",
    "    \n",
    "    #model3=GNN(input_size=3, hidden_channels=3)\n",
    "    learning_rate = 0.001\n",
    "    # Definir la función de pérdida y el optimizador\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Listas para almacenar las pérdidas en cada época\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    #num_epochs = 12\n",
    "    model.train()  # Cambiar al modo de entrenamiento\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for data in dataset:\n",
    "            x = data.x  # Características de los nodos\n",
    "            edge_index = data.edge_index\n",
    "            y = data.y#.view(-1)#.to(device)  # Etiquetas o clases\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x=x, edge_index=edge_index)\n",
    "            loss = loss_fn(logits, y)\n",
    "            loss.backward()  # Realizar el paso de atrás (backward)\n",
    "            optimizer.step()  # Actualizar los pesos del modelo\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calcular la pérdida promedio en cada época\n",
    "        epoch_loss = running_loss / len(dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Realizar la validación del modelo en cada época\n",
    "        model.eval()  # Cambiar al modo de evaluación\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_val_loss = 0.0\n",
    "\n",
    "            for data in data_test:\n",
    "                x_val = data.x\n",
    "                edge_index_val = data.edge_index\n",
    "                y_val = data.y#.view(-1)#.to(device)\n",
    "\n",
    "                val_logits = model(x=x_val, edge_index=edge_index_val)\n",
    "                val_loss = loss_fn(val_logits, y_val)\n",
    "\n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "            # Calcular la pérdida promedio en la validación\n",
    "            val_epoch_loss = running_val_loss / len(data_test)\n",
    "            val_losses.append(val_epoch_loss)\n",
    "\n",
    "        # Imprimir información del progreso del entrenamiento\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}')\n",
    "\n",
    "    # Crear las curvas de aprendizaje\n",
    "    plot_learning_curves(train_losses, val_losses)\n",
    "    \n",
    "def testing(model, dataset):\n",
    "    model.eval()  # Set the model in evaluation mode\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    predictions_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            x = data.x  # Características de los nodo\n",
    "            edge_index = data.edge_index\n",
    "            y = data.y.unsqueeze(0)#.view(-1)#.to(device)  # Etiquetas o clases\n",
    "\n",
    "\n",
    "            predictions = model(x=x, edge_index=edge_index)  # Forward pass\n",
    "\n",
    "            loss = loss_fn(predictions, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions_list.append(predictions.detach().cpu().numpy())\n",
    "            labels_list.append(y.detach().cpu().numpy())\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions_array = np.concatenate(predictions_list, axis=0)\n",
    "        labels_array = np.concatenate(labels_list, axis=0)\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(labels_array, predictions_array)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(labels_array, predictions_array)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = mean_squared_error(labels_array, predictions_array, squared=False)\n",
    "\n",
    "        # Calculate R-squared\n",
    "        r2 = r2_score(labels_array, predictions_array)\n",
    "\n",
    "#         print(\"MSE:\", mse)\n",
    "#         print(\"MAE:\", mae)\n",
    "#         print(\"RMSE:\", rmse)\n",
    "#         print(\"R-squared:\", r2)\n",
    "\n",
    "#         #print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "#         # print(\"Mean Squared Error (MSE): {:.4f}\".format(mse))\n",
    "#         # print(\"R-squared (R²): {:.4f}\".format(r2))\n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.scatter(labels_array, predictions_array)\n",
    "#         ax.axline((0, 0), slope=1, color='red')\n",
    "#         # Add labels and title\n",
    "#         plt.xlabel(\"Labels\")\n",
    "#         plt.ylabel(\"Predictions\")\n",
    "#         plt.title(\"Predictions vs. Labels\")\n",
    "        \n",
    "\n",
    "#         # Display the plot\n",
    "#         plt.show()\n",
    "        \n",
    "        return mse, mae, rmse, r2\n",
    "        \n",
    "def cross_validate(model, dataset, num_folds=5, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for a given model and dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to evaluate.\n",
    "        dataset (list): The dataset containing data for cross-validation.\n",
    "        num_folds (int): The number of folds for cross-validation.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    all_mse = []\n",
    "    all_mae = []\n",
    "    all_rmse = []\n",
    "    all_r2 = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(dataset):\n",
    "        # Split the dataset into training and validation sets for this fold\n",
    "        train_set = [dataset[i] for i in train_idx] \n",
    "        val_set = [dataset[i] for i in val_idx]\n",
    "\n",
    "        # Train the model on the training set\n",
    "        train(model, num_epochs, train_set, val_set)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        mse, mae, rmse, r2 = testing(model, val_set)\n",
    "\n",
    "        all_mse.append(mse)\n",
    "        all_mae.append(mae)\n",
    "        all_rmse.append(rmse)\n",
    "        all_r2.append(r2)\n",
    "\n",
    "    # Calculate and return the mean of evaluation metrics across all folds\n",
    "    mean_mse = np.mean(all_mse)\n",
    "    mean_mae = np.mean(all_mae)\n",
    "    mean_rmse = np.mean(all_rmse)\n",
    "    mean_r2 = np.mean(all_r2)\n",
    "\n",
    "    return mean_mse, mean_mae, mean_rmse, mean_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ca6b3",
   "metadata": {},
   "source": [
    "## 2. Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "860c6235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\l.sanchezparra\\AppData\\Local\\Temp\\ipykernel_31044\\1382544550.py:39: DeprecationWarning: \n",
      "\n",
      "The scipy.sparse array containers will be used instead of matrices\n",
      "in Networkx 3.0. Use `to_scipy_sparse_array` instead.\n",
      "  adj_matrix = nx.convert_matrix.to_scipy_sparse_matrix(graph)\n"
     ]
    }
   ],
   "source": [
    "training = load_gpickle_files(r\"C:\\Users\\l.sanchezparra\\Documents\\TFG\\Aprendizaje_de_la_entropia_de_un_grafo_usando_GNN\\data\\train\", 1)  # Carga los datos con la función load_gpickle_files\n",
    "validation = load_gpickle_files(r\"C:\\Users\\l.sanchezparra\\Documents\\TFG\\Aprendizaje_de_la_entropia_de_un_grafo_usando_GNN\\data\\validation\", 1)  # Carga los datos con la función load_gpickle_files\n",
    "test = load_gpickle_files(r\"C:\\Users\\l.sanchezparra\\Documents\\TFG\\Aprendizaje_de_la_entropia_de_un_grafo_usando_GNN\\data\\test\", 1)  # Carga los datos con la función load_gpickle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f087bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = training + validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acaeefc",
   "metadata": {},
   "source": [
    "## 3. Modelos GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3e498",
   "metadata": {},
   "source": [
    "### 3.1 One GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d310e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneGCNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_channels):\n",
    "        super(OneGCNLayer, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = GCNConv(input_size, hidden_channels)\n",
    "        \n",
    "        \n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch = None,  edge_col = None):\n",
    "        \n",
    "        # Node embedding \n",
    "        x = self.conv1(x, edge_index, edge_col)\n",
    "               \n",
    "        # Readout layer\n",
    "        batch = torch.zeros(x.shape[0],dtype=int) if batch is None else batch\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed65aa",
   "metadata": {},
   "source": [
    "### 3.2 Two GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14708ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGCNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_channels):\n",
    "        super(TwoGCNLayer, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = GCNConv(input_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch = None,  edge_col = None):\n",
    "        \n",
    "        # Node embedding \n",
    "        x = self.conv1(x, edge_index, edge_col)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_col)\n",
    "        \n",
    "        # Readout layer\n",
    "        batch = torch.zeros(x.shape[0],dtype=int) if batch is None else batch\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702a768",
   "metadata": {},
   "source": [
    "### 3.3 Three GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86151c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeGCNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_channels):\n",
    "        super(ThreeGCNLayer, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = GCNConv(input_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch = None,  edge_col = None):\n",
    "        \n",
    "        # Node embedding \n",
    "        x = self.conv1(x, edge_index, edge_col)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_col)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_col)\n",
    "               \n",
    "        # Readout layer\n",
    "        batch = torch.zeros(x.shape[0],dtype=int) if batch is None else batch\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf6b78",
   "metadata": {},
   "source": [
    "### 3.4 Four GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8234eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourGCNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_channels):\n",
    "        super(FourGCNLayer, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.conv1 = GCNConv(input_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        \n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch = None,  edge_col = None):\n",
    "        \n",
    "        # Node embedding \n",
    "        x = self.conv1(x, edge_index, edge_col)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_col)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_col)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index, edge_col)\n",
    "               \n",
    "        # Readout layer\n",
    "        batch = torch.zeros(x.shape[0],dtype=int) if batch is None else batch\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181d091",
   "metadata": {},
   "source": [
    "## 4. Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391078b9",
   "metadata": {},
   "source": [
    "### 4.1 Experimento 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe36c9",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02571d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\l.sanchezparra\\AppData\\Local\\Temp\\ipykernel_31044\\1382544550.py:39: DeprecationWarning: \n",
      "\n",
      "The scipy.sparse array containers will be used instead of matrices\n",
      "in Networkx 3.0. Use `to_scipy_sparse_array` instead.\n",
      "  adj_matrix = nx.convert_matrix.to_scipy_sparse_matrix(graph)\n"
     ]
    }
   ],
   "source": [
    "training = load_gpickle_files(r\"C:\\Users\\l.sanchezparra\\Documents\\TFG\\Aprendizaje_de_la_entropia_de_un_grafo_usando_GNN\\data\\train\", 1)  # Carga los datos con la función load_gpickle_files\n",
    "validation = load_gpickle_files(r\"C:\\Users\\l.sanchezparra\\Documents\\TFG\\Aprendizaje_de_la_entropia_de_un_grafo_usando_GNN\\data\\validation\", 1)  # Carga los datos con la función load_gpickle_files\n",
    "test = load_gpickle_files(r\"C:\\Users\\l.sanchezparra\\Documents\\TFG\\Aprendizaje_de_la_entropia_de_un_grafo_usando_GNN\\data\\test\", 1)  # Carga los datos con la función load_gpickle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8e658b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = training + validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7cbc2",
   "metadata": {},
   "source": [
    "#### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9dd6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "model11=OneGCNLayer(input_size=1, hidden_channels=1)\n",
    "model12=TwoGCNLayer(input_size=1, hidden_channels=1)\n",
    "model13=ThreeGCNLayer(input_size=1, hidden_channels=1)\n",
    "model14=FourGCNLayer(input_size=1, hidden_channels=1)\n",
    "\n",
    "models = [(\"OneGCNLayer\" , model11), \n",
    "          (\"TwoGCNLayer\" , model12), \n",
    "          (\"ThreeGCNLayer\" ,model13),\n",
    "          (\"FourGCNLayer\", model14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c621214a",
   "metadata": {},
   "source": [
    "#### Entrenamiento y cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c968b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\l.sanchezparra\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.1165, Val Loss: 0.0717\n",
      "Epoch [2/10], Train Loss: 0.0705, Val Loss: 0.0683\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=[\"Model\", \"MSE\", \"MAE\", \"R-squared\"])\n",
    "for model_name, model in models:\n",
    "    mean_mse, mean_mae, mean_rmse, mean_r2 =cross_validate(model, dataset, num_folds=5, num_epochs=10)\n",
    "    results_df = results_df.append(\n",
    "        {\"Model\": model_name, \"MSE\": mse_mean, \"MAE\": mae_mean, \"R-squared\": r2_mean},\n",
    "        ignore_index=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
